<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Akshat Gupta</title>
  
  <meta name="author" content="Akshat Gupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <script type="text/javascript" src="script.js"></script>
</head>

<br><br>
<body onload="startGame()">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Akshat Gupta</name>
              </p>
              <p>
                I am a third-year PhD student at UC Berkeley affiliated with <a href="https://bair.berkeley.edu">BAIR</a> and Berkeley Speech Group, advised by <a href="http://people.eecs.berkeley.edu/~gopala/">Gopala Anumanchipalli</a>. 
                Before joining Berkeley, I spent two wonderful years at <a href="https://www.jpmorgan.com/technology/artificial-intelligence">AI Research, JPMorgan</a> where I worked as an NLP Research Engineer. 
                <br><br>
                <!--I graduated from Carnegie Mellon University (MS), where I was advised by Prof. <a href="http://festvox.org/awb/">Alan Black</a>. 
                In another life, I used to study physics. I did my Masters in Physics from Technical University of Munich, Germany, and my thesis was advised by Prof. <a href="https://www.ams.jhu.edu/~eyink/">Gregory Eyink</a> at Johns Hopkins University.
                <br><br>-->
                Currently I am actively working on OR constantly thinking about the following:
                <ul>
                    <li> Continual Learning </li>
                    <li> Interpretability [<a href="https://arxiv.org/abs/2510.18871">Arxiv</a>, <a href="https://openreview.net/forum?id=JiL0sYCGeP">ACL '25 SRW</a>, <a href="https://arxiv.org/abs/2306.07384">BlackboxNLP '23</a>]</li>
                    <li> AI Scientist </li>
                    <li>LLMs and Poker [<a href="https://arxiv.org/abs/2501.08328">AAAI '25</a>, <a href="https://arxiv.org/abs/2308.12466">Arxiv</a>]</li>
                </ul>

                In the past, I've worked on the following topics:
                <ul>
                    <li>Lifelong Knowledge editing [<a href="https://arxiv.org/abs/2506.04226">ACL '25</a>, <a href="https://arxiv.org/abs/2403.07175">EMNLP '24</a>, <a href="https://arxiv.org/abs/2502.01636">EMNLP '25 Findings</a>, <a href="https://arxiv.org/abs/2403.14236">EMNLP '24 Findings</a>, <a href="https://arxiv.org/abs/2401.07453">ACL '24 Findings</a>]</li>
                    <li>Generative Spoken Language Modelling [<a href="https://arxiv.org/abs/2410.07168">ICLR '25</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9415112">ICASSP '21</a>, <a href="https://arxiv.org/abs/2110.09264">Interspeech '22</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9688264">ASRU '21</a>]</li>
                    <li>Personality Measurement in LLMs [<a href="https://arxiv.org/abs/2309.08163">BlackboxNLP '24</a>, <a href="https://arxiv.org/abs/2305.14693">Preprint 1</a>, <a href="https://arxiv.org/abs/2402.14805">Preprint 2</a>]</li>
                    <!--<li>Personality Measurement in LLMs (<a href="https://arxiv.org/abs/2309.08163">1</a>, <a href="https://arxiv.org/abs/2305.14693">2</a>, <a href="https://arxiv.org/abs/2402.14805">3</a>)</li>-->
                </ul>

                <u>Feel free to reach out to me</u> to discuss about any of my current or past interests, or anything related to AI and technology. Lately, my musings have been about predicting the future and I love hearing people's thoughts on it!


                <!--Please reach out if you're interested in these topics and want to chat.
                <b>Note for Students and Potential Collaborators</b> : I love collaborations and have <a href="https://scalable-model-editing.github.io/team.html">mentored many students</a> on different research projects in the past. Please feel free to reach out to me to discuss ideas, my previous or current projects or collaborations.-->
              </p>
              <p style="text-align:center">
                <a href="mailto:akshat.gupta@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=v80j6o0AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://x.com/akshatgupta57">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/akshat57/">Linkedin</a> &nbsp/&nbsp
                <a href="./data/CV_Akshat.pdf" target="_blank">CV (Updated Aug 2025)</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_akshat_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_akshat_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <section id="news">
          <h2 class="news-title">News</h2>
          <ul>
              <li>Oct 22, 2025 - <a href="https://arxiv.org/abs/2510.18871">"How LLMs use their Depth?"</a> is released on Arxiv. This AI-generated <a href="https://www.youtube.com/watch?v=y9_zyYGMj9o&t=39s">video</a> does a great job summarizing our work.</li>
              <li>Aug 20, 2025 - <a href="https://arxiv.org/abs/2502.01636">"Lifelong Knowledge Editing requires Better Regularization"</a> accepted to <b>EMNLP 2025 Findings</b></li>
              <li>May 19, 2025 - Started internship at IBM Research.</li>
              <li>May 15, 2025 - <a href="https://arxiv.org/abs/2506.04226">"Efficient Knowledge Editing via Minimal Precomputation"</a> accepted to <b>ACL 2025 Main Conference</b></li>
              <li>March 4, 2025 - <span style="color: red;"> <b>Outstanding Paper Award</b> at the <u><a style="color: red;" href="https://knowledgeable-lm.github.io/">KnowFM Workshop @ AAAI 2025</a></u>. (<u><a style="color: red;" href="https://arxiv.org/abs/2502.19416">Paper Link</a></u>) </span> </li>
              <li>Feb 2025 - <span>I will be attending AAAI 2025 in Philadephia from Feb 26 - March 4. Please reach out if you want to chat about interpretability, reasoning or knowlegde editing.</span> </li>
              <li>Jan 22, 2025 - <a href="https://arxiv.org/abs/2410.07168">"Sylber: Syllabic Embedding Representation of Speech from Raw Audio"</a> accepted to <b>ICLR 2025</b></li>
              <li>Dec 9, 2024 - <a href="https://arxiv.org/abs/2501.08328">"PokerBench: Training LLMs to become Professional Poker Players"</a> accepted to <b>AAAI 2025</b></li>
              <li>Nov 2024 - <span>I will be attending EMNLP 2024 in Miami from November 11 - 16. Please reach out if you want to chat. </span> </li>
              <li>Sep 20, 2024 - <a href="https://arxiv.org/abs/2403.07175">"Rebuilding ROME : Resolving Model Collapse during Sequential Editing"</a> accepted to <b>EMNLP 2024 Main Conference</b></li>
              <li>Sep 20, 2024 - <a href="https://arxiv.org/abs/2403.14236">"A Unified Framework for Model Editing"</a> accepted to <b>EMNLP 2024 Findings</b></li>
              <li>Sep 20, 2024 - <a href="https://arxiv.org/abs/2309.08163">"Self-Assessment Tests are Unreliable Measures of LLM Personality"</a> accepted to <b>BlackboxNLP 2024</b>, co-located with EMNLP 2024</li>
              <li >August 12, 2024 - <span>Attending ACL 2024 in Bangkok!!</span></li>
              <li>May 15, 2024 - <a href="https://arxiv.org/abs/2401.07453">"Model Editing at Scale leads to Gradual and Catastrophic Forgetting"</a> accepted to <b>ACL 2024 Findings</b></li>
              <!--<<li>May 2, 2024 - <a href="https://arxiv.org/abs/2405.00664">"Is Bigger Edit Batch Size Always Better?- An Empirical Study on Model Editing with Llama-3"</a> picked by <a href="https://twitter.com/_akhaliq/status/1785869426924966190"> AK</a> as part of <a href="https://huggingface.co/papers/2405.00664"> Huggingface Daily Papers</a></li>-->
              <!--<li>March 21, 2024 - <a href="https://arxiv.org/abs/2403.14236">"A Unified Framework for Model Editing"</a> released on arxiv</li>
              <li>March 11, 2024 - <a href="https://arxiv.org/abs/2403.07175">"Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing"</a> released on arxiv</li>
              <li>January 15, 2024 - <a href="https://arxiv.org/abs/2401.07453">"Model Editing at Scale leads to Gradual and Catastrophic Forgetting"</a> released on arxiv</li>-->
          </ul>
      </section>
        
        
</body>

</html>
